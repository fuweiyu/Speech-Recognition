{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPcbliDiKsAq"
      },
      "outputs": [],
      "source": [
        "#  Install necessary libraries\n",
        "!pip install transformers torch torchaudio pyaudio pydub huggingface_hub librosa soundfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I seems that some librareis are needed in order to install pyaudio.\n",
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
        "!pip install PyAudio"
      ],
      "metadata": {
        "id": "UfSwRpnIMZaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pydub couldn't be installed in the above block, so I add it now here\n",
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUfNyfU_M3WI",
        "outputId": "e560faa1-78ce-49ee-ee4c-82e8120dda2e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizer\n",
        "import pyaudio\n",
        "import wave\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "from IPython.display import Audio\n",
        "import os\n"
      ],
      "metadata": {
        "id": "DZSBhqhuK44t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Loading model and evaluating"
      ],
      "metadata": {
        "id": "wikY7LTtLAEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load  fine-tuned model from Hugging Face\n",
        "# Replace 'your-username/your-repo-name' with your Hugging Face model repository ID\n",
        "MODEL_ID = \"your-username/your-repo-name\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the processor (combines feature extractor and tokenizer)\n",
        "# Using WhisperProcessor is often more convenient\n",
        "try:\n",
        "    processor = WhisperProcessor.from_pretrained(MODEL_ID)\n",
        "    model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID).to(DEVICE)\n",
        "    print(f\"Successfully loaded model and processor for {MODEL_ID}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading processor. Trying feature extractor and tokenizer separately: {e}\")\n",
        "    # Fallback to loading tokenizer and feature extractor separately if processor fails\n",
        "    # This is closer to your original training script's setup\n",
        "    tokenizer = WhisperTokenizer.from_pretrained(MODEL_ID, language=\"spanish\", task=\"transcribe\") # Or your target language e.g. \"gn\"\n",
        "    feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_ID)\n",
        "    model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID).to(DEVICE)\n",
        "    # You might need to create a processor manually if you go this route for the pipeline later\n",
        "    # processor = WhisperProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
        "    print(f\"Successfully loaded model, tokenizer, and feature extractor for {MODEL_ID}\")\n",
        "\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "# Regarding language for generation:\n",
        "# Your training script used 'spanish' in model.generate().\n",
        "# For best results with your Guarani fine-tuned model, you might want to specify Guarani.\n",
        "# Whisper uses language codes (e.g., 'gn' for Guarani if your tokenizer was adapted or supports it).\n",
        "# If you used 'spanish' as a target language token during fine-tuning consistently, you might stick to it.\n",
        "# You can set this in the generation config or pass it to the generate function.\n",
        "# Example:\n",
        "# model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"gn\", task=\"transcribe\")\n",
        "# Or if your processor was loaded with Spanish as default and fine-tuning adapted it for Guarani without changing the language token:\n",
        "# model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"spanish\", task=\"transcribe\")\n",
        "# For now, let's assume the loaded model's config is appropriate or you'll pass language to generate/pipeline."
      ],
      "metadata": {
        "id": "B13NiPIZK6ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2. Getting audio as input"
      ],
      "metadata": {
        "id": "6TVkM0VuNJLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Record audio for a fixed duration\n",
        "FORMAT = pyaudio.paInt16  # Audio format (16-bit PCM)\n",
        "CHANNELS = 1              # Number of audio channels (1 for mono, 2 for stereo)\n",
        "RATE = 16000              # Sample rate (Whisper expects 16kHz)\n",
        "CHUNK = 1024              # Number of frames per buffer\n",
        "RECORD_SECONDS = 5        # Duration of recording in seconds\n",
        "WAV_FILENAME = \"recorded_audio.wav\"\n",
        "MP3_FILENAME = \"recorded_audio.mp3\"\n",
        "\n",
        "audio = pyaudio.PyAudio()\n",
        "\n",
        "# Start Recording\n",
        "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
        "                    rate=RATE, input=True,\n",
        "                    frames_per_buffer=CHUNK)\n",
        "\n",
        "print(\"Recording...\")\n",
        "frames = []\n",
        "\n",
        "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
        "    data = stream.read(CHUNK)\n",
        "    frames.append(data)\n",
        "\n",
        "print(\"Finished recording.\")\n",
        "\n",
        "# Stop Recording\n",
        "stream.stop_stream()\n",
        "stream.close()\n",
        "audio.terminate()\n",
        "\n",
        "# Save the recorded data as a WAV file\n",
        "wf = wave.open(WAV_FILENAME, 'wb')\n",
        "wf.setnchannels(CHANNELS)\n",
        "wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
        "wf.setframerate(RATE)\n",
        "wf.writeframes(b''.join(frames))\n",
        "wf.close()\n",
        "\n",
        "print(f\"Audio saved as {WAV_FILENAME}\")\n",
        "\n",
        "# Convert WAV to MP3 using pydub\n",
        "try:\n",
        "    sound = AudioSegment.from_wav(WAV_FILENAME)\n",
        "    sound.export(MP3_FILENAME, format=\"mp3\")\n",
        "    print(f\"Converted to {MP3_FILENAME}\")\n",
        "    os.remove(WAV_FILENAME) # Optional: remove the intermediate WAV file\n",
        "    # Display the audio player in the notebook\n",
        "    display(Audio(MP3_FILENAME))\n",
        "except Exception as e:\n",
        "    print(f\"Could not convert to MP3. Make sure ffmpeg is installed and in your PATH. Error: {e}\")\n",
        "    print(\"You can still use the WAV file for prediction if MP3 conversion fails, but ensure your prediction block loads the correct file type.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "bLd2bA2FNjOa",
        "outputId": "a38711c5-67e2-43a3-9562-a275e926cdf1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno -9996] Invalid input device (no default output device)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-97639f0a42fa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Start Recording\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m stream = audio.open(format=FORMAT, channels=CHANNELS,\n\u001b[0m\u001b[1;32m     14\u001b[0m                     \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     frames_per_buffer=CHUNK)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyaudio/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPyAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStream\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \"\"\"\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_streams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyaudio/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, PA_manager, rate, channels, format, input, output, input_device_index, output_device_index, frames_per_buffer, start, input_host_api_specific_stream_info, output_host_api_specific_stream_info, stream_callback)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;31m# calling pa.open returns a stream object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_latency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputLatency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno -9996] Invalid input device (no default output device)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Record audio (stop with Enter key in console - more for script, but can work)\n",
        "# This method will record until you press Enter in the console where Jupyter is running.\n",
        "# Note: In some environments (like Colab without specific widgets), direct key press detection is hard.\n",
        "# A simpler alternative is just a longer fixed duration.\n",
        "\n",
        "FORMAT = pyaudio.paInt16\n",
        "CHANNELS = 1\n",
        "RATE = 16000 # Whisper expects 16kHz\n",
        "CHUNK = 1024\n",
        "WAV_FILENAME_MANUAL = \"recorded_audio_manual.wav\"\n",
        "MP3_FILENAME_MANUAL = \"recorded_audio_manual.mp3\"\n",
        "\n",
        "audio = pyaudio.PyAudio()\n",
        "\n",
        "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
        "                    rate=RATE, input=True,\n",
        "                    frames_per_buffer=CHUNK)\n",
        "\n",
        "print(\"Recording... Press Enter in the console running Jupyter to stop.\")\n",
        "frames = []\n",
        "# This input() will block, so recording happens while it waits.\n",
        "# This isn't a perfect \"stop recording on keypress\" but a simple way to gate it.\n",
        "# For true keypress stop, you'd need a more complex GUI or threading.\n",
        "try:\n",
        "    while True: # A loop to simulate continuous recording until input\n",
        "        data = stream.read(CHUNK)\n",
        "        frames.append(data)\n",
        "        # This is a crude way to check for input without blocking read too much\n",
        "        # It's not ideal. A better way is to record fixed long duration or use threads.\n",
        "        # For this example, let's make it simpler: Record for a max duration or until input.\n",
        "        # The input() below is the primary stop.\n",
        "except KeyboardInterrupt: # If you press Ctrl+C in console\n",
        "    print(\"Recording stopped by user.\")\n",
        "except Exception as e:\n",
        "    pass # Handle other potential stream errors\n",
        "\n",
        "# The following is a simpler approach for notebooks:\n",
        "# Ask user to press enter, then record for a few seconds.\n",
        "# Or, just use a fixed duration like Method 1.\n",
        "\n",
        "# Let's refine Method 2 for a notebook: record after user hits enter.\n",
        "print(\"Prepare to record. Press Enter to start recording for 10 seconds.\")\n",
        "input() # Wait for user to press Enter\n",
        "\n",
        "print(\"Recording for 10 seconds...\")\n",
        "frames = []\n",
        "for i in range(0, int(RATE / CHUNK * 10)): # Record for 10 seconds\n",
        "    data = stream.read(CHUNK)\n",
        "    frames.append(data)\n",
        "print(\"Finished recording.\")\n",
        "\n",
        "stream.stop_stream()\n",
        "stream.close()\n",
        "audio.terminate()\n",
        "\n",
        "wf = wave.open(WAV_FILENAME_MANUAL, 'wb')\n",
        "wf.setnchannels(CHANNELS)\n",
        "wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
        "wf.setframerate(RATE)\n",
        "wf.writeframes(b''.join(frames))\n",
        "wf.close()\n",
        "print(f\"Audio saved as {WAV_FILENAME_MANUAL}\")\n",
        "\n",
        "try:\n",
        "    sound = AudioSegment.from_wav(WAV_FILENAME_MANUAL)\n",
        "    sound.export(MP3_FILENAME_MANUAL, format=\"mp3\")\n",
        "    print(f\"Converted to {MP3_FILENAME_MANUAL}\")\n",
        "    os.remove(WAV_FILENAME_MANUAL)\n",
        "    display(Audio(MP3_FILENAME_MANUAL))\n",
        "except Exception as e:\n",
        "    print(f\"Could not convert to MP3: {e}. Using WAV.\")\n",
        "    MP3_FILENAME_MANUAL = WAV_FILENAME_MANUAL # Fallback to WAV if MP3 fails"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Vff6cvX_NJSf",
        "outputId": "0febfea5-cbbd-4ef8-df4a-374fb233fe68"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno -9996] Invalid input device (no default output device)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ac91ec607526>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m stream = audio.open(format=FORMAT, channels=CHANNELS,\n\u001b[0m\u001b[1;32m     16\u001b[0m                     \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     frames_per_buffer=CHUNK)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyaudio/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPyAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStream\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \"\"\"\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_streams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyaudio/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, PA_manager, rate, channels, format, input, output, input_device_index, output_device_index, frames_per_buffer, start, input_host_api_specific_stream_info, output_host_api_specific_stream_info, stream_callback)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;31m# calling pa.open returns a stream object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_latency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputLatency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno -9996] Invalid input device (no default output device)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3. Geting predictions with new model"
      ],
      "metadata": {
        "id": "e3pYHOEFNvI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Make predictions with the recorded audio\n",
        "# Ensure this uses the filename from the recording block you ran (e.g., MP3_FILENAME)\n",
        "AUDIO_TO_TRANSCRIBE = MP3_FILENAME # Or MP3_FILENAME_MANUAL if you used that block\n",
        "\n",
        "if not os.path.exists(AUDIO_TO_TRANSCRIBE):\n",
        "    print(f\"Audio file {AUDIO_TO_TRANSCRIBE} not found. Please record audio first.\")\n",
        "else:\n",
        "    print(f\"Transcribing {AUDIO_TO_TRANSCRIBE}...\")\n",
        "    # Load the audio file using librosa (handles resampling and conversion to mono float)\n",
        "    # Whisper feature_extractor expects a 1D numpy array at 16kHz.\n",
        "    speech_array, sampling_rate = librosa.load(AUDIO_TO_TRANSCRIBE, sr=16000, mono=True)\n",
        "\n",
        "    # Preprocess the audio\n",
        "    # If you loaded processor = WhisperProcessor.from_pretrained(MODEL_ID)\n",
        "    inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "    input_features = inputs.input_features.to(DEVICE)\n",
        "    # If you loaded feature_extractor and tokenizer separately:\n",
        "    # input_features = feature_extractor(speech_array, sampling_rate=16000, return_tensors=\"pt\").input_features.to(DEVICE)\n",
        "\n",
        "\n",
        "    # Generate token IDs\n",
        "    with torch.no_grad():\n",
        "        # You might need to specify the language here if your model requires it for Guarani.\n",
        "        # Example: generated_ids = model.generate(input_features, language=\"gn\")\n",
        "        # If your model was trained with \"spanish\" as the target token:\n",
        "        # generated_ids = model.generate(input_features, language=\"spanish\")\n",
        "        # Or rely on the model's default config if set:\n",
        "        generated_ids = model.generate(input_features)\n",
        "\n",
        "    # Decode the token IDs to text\n",
        "    # If you loaded processor:\n",
        "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    # If you loaded tokenizer separately:\n",
        "    # transcription = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    print(\"\\nTranscription:\")\n",
        "    print(transcription)\n",
        "\n",
        "    # Alternative: Using the pipeline (can be more convenient)\n",
        "    # from transformers import pipeline\n",
        "    # print(\"\\nUsing pipeline for transcription:\")\n",
        "    # # If you loaded feature_extractor and tokenizer separately, ensure processor is created for pipeline\n",
        "    # # if 'processor' not in locals():\n",
        "    # # processor = WhisperProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
        "\n",
        "    # pipe = pipeline(\"automatic-speech-recognition\",\n",
        "    #                 model=model,\n",
        "    #                 tokenizer=processor.tokenizer, # or your loaded tokenizer\n",
        "    #                 feature_extractor=processor.feature_extractor, # or your loaded feature_extractor\n",
        "    #                 device=DEVICE)\n",
        "    # result = pipe(AUDIO_TO_TRANSCRIBE, generate_kwargs={\"language\": \"gn\"}) # or \"spanish\" or remove if config is set\n",
        "    # print(result[\"text\"])"
      ],
      "metadata": {
        "id": "diqTg76pNvQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4. Comparing model sizes"
      ],
      "metadata": {
        "id": "2RSKvi0vN3nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Load a specific older version of the model (your 'tiny' version)\n",
        "# Replace 'your-username/your-repo-name' with your Hugging Face model ID\n",
        "# Replace 'commit_hash_of_tiny_version' with the actual commit hash from Hugging Face\n",
        "MODEL_ID_FOR_COMPARISON = \"your-username/your-repo-name\"\n",
        "COMMIT_HASH_TINY = \"commit_hash_of_tiny_version\" # <--- PASTE THE CORRECT COMMIT HASH HERE\n",
        "\n",
        "print(f\"Loading 'tiny' version from commit: {COMMIT_HASH_TINY}\")\n",
        "\n",
        "try:\n",
        "    # Attempt to load with WhisperProcessor first\n",
        "    processor_tiny = WhisperProcessor.from_pretrained(MODEL_ID_FOR_COMPARISON, revision=COMMIT_HASH_TINY)\n",
        "    model_tiny = WhisperForConditionalGeneration.from_pretrained(MODEL_ID_FOR_COMPARISON, revision=COMMIT_HASH_TINY).to(DEVICE)\n",
        "    print(\"Successfully loaded 'tiny' model and processor using revision.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading 'tiny' processor with revision: {e}. Trying tokenizer/feature_extractor separately.\")\n",
        "    # Fallback if processor loading fails for that revision\n",
        "    tokenizer_tiny = WhisperTokenizer.from_pretrained(MODEL_ID_FOR_COMPARISON, revision=COMMIT_HASH_TINY)\n",
        "    feature_extractor_tiny = WhisperFeatureExtractor.from_pretrained(MODEL_ID_FOR_COMPARISON, revision=COMMIT_HASH_TINY)\n",
        "    model_tiny = WhisperForConditionalGeneration.from_pretrained(MODEL_ID_FOR_COMPARISON, revision=COMMIT_HASH_TINY).to(DEVICE)\n",
        "    print(\"Successfully loaded 'tiny' model, tokenizer, and feature_extractor using revision.\")\n",
        "\n",
        "\n",
        "model_tiny.eval()\n",
        "\n",
        "# Now you have 'model_tiny' (and its processor/tokenizer/feature_extractor) loaded.\n",
        "# You can compare its performance to your current 'model' (the 'small' version).\n",
        "# For example, you could re-run the prediction cell (Cell 6) but use model_tiny:\n",
        "\n",
        "# Example of using the loaded tiny model for prediction:\n",
        "# if 'speech_array' in locals(): # Check if speech_array from previous prediction exists\n",
        "#     inputs_tiny = processor_tiny(speech_array, sampling_rate=16000, return_tensors=\"pt\").input_features.to(DEVICE)\n",
        "#     # Or if using separate tokenizer/feature_extractor_tiny:\n",
        "#     # inputs_tiny = feature_extractor_tiny(speech_array, sampling_rate=16000, return_tensors=\"pt\").input_features.to(DEVICE)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         generated_ids_tiny = model_tiny.generate(inputs_tiny) # Add language if needed\n",
        "#     transcription_tiny = processor_tiny.batch_decode(generated_ids_tiny, skip_special_tokens=True)[0]\n",
        "#     # Or with separate tokenizer_tiny:\n",
        "#     # transcription_tiny = tokenizer_tiny.batch_decode(generated_ids_tiny, skip_special_tokens=True)[0]\n",
        "#     print(\"\\nTranscription from 'tiny' model:\")\n",
        "#     print(transcription_tiny)\n",
        "# else:\n",
        "#     print(\"Run audio recording and preprocessing first to compare 'tiny' model.\")"
      ],
      "metadata": {
        "id": "qtfc-B0uN3t2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}