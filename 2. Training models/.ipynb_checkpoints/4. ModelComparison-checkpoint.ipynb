{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part 0. Package setting\n",
    "The way to do this is to manually chekc how to install the packages in your environment. \n",
    "this worked on collab but I installed the packages manually in my machine, the porcedure to get them will likely ghe different depending on your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPcbliDiKsAq"
   },
   "outputs": [],
   "source": [
    "#  Install necessary libraries\n",
    "!pip install transformers torch torchaudio pyaudio pydub huggingface_hub librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UfSwRpnIMZaE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mError: \u001b[0m\u001b[1mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
      "\u001b[1;31mError: \u001b[0m\u001b[1mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting PyAudio\n",
      "  Using cached PyAudio-0.2.14.tar.gz (47 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pydub\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Building wheels for collected packages: PyAudio\n",
      "  Building wheel for PyAudio (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for PyAudio \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[27 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-build-env-hh9xesxp/overlay/lib/python3.13/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         License :: OSI Approved :: MIT License\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   self._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-313/pyaudio\n",
      "  \u001b[31m   \u001b[0m copying src/pyaudio/__init__.py -> build/lib.linux-x86_64-cpython-313/pyaudio\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'pyaudio._portaudio' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-313/src/pyaudio\n",
      "  \u001b[31m   \u001b[0m gcc -fno-strict-overflow -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -fexceptions -fcf-protection -fexceptions -fcf-protection -fexceptions -fcf-protection -O3 -fPIC -I/usr/local/include -I/usr/include -I/usr/include/python3.13 -c src/pyaudio/device_api.c -o build/temp.linux-x86_64-cpython-313/src/pyaudio/device_api.o\n",
      "  \u001b[31m   \u001b[0m src/pyaudio/device_api.c:9:10: fatal error: portaudio.h: No such file or directory\n",
      "  \u001b[31m   \u001b[0m     9 | #include \"portaudio.h\"\n",
      "  \u001b[31m   \u001b[0m       |          ^~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m compilation terminated.\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/gcc' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for PyAudio\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build PyAudio\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (PyAudio)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# I seems that some libraries are needed in order to install pyaudio.\n",
    "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
    "!pip install PyAudio pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DZSBhqhuK44t"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizer\n",
    "import pyaudio\n",
    "import wave\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from IPython.display import Audio\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wikY7LTtLAEh"
   },
   "source": [
    "## Part 1: Loading model and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B13NiPIZK6ty"
   },
   "outputs": [],
   "source": [
    "# Load  fine-tuned model from Hugging Face\n",
    "MODEL_ID = \"SchindleriaPraematurus/whisper-tiny-gn-finetuned\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the processor (combines feature extractor and tokenizer)\n",
    "try:\n",
    "    processor = WhisperProcessor.from_pretrained(MODEL_ID)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    print(f\"Successfully loaded model and processor for {MODEL_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading processor. Trying feature extractor and tokenizer separately: {e}\")\n",
    "    # Fallback to loading tokenizer and feature extractor separately if processor fails\n",
    "    # This is closer to your original training script's setup\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(MODEL_ID, language=\"spanish\", task=\"transcribe\") \n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_ID)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    # processor = WhisperProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "    print(f\"Successfully loaded model, tokenizer, and feature extractor for {MODEL_ID}\")\n",
    "\n",
    "model.eval() # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TVkM0VuNJLN"
   },
   "source": [
    "## Part 2. Getting audio as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "bLd2bA2FNjOa",
    "outputId": "a38711c5-67e2-43a3-9562-a275e926cdf1"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno -9996] Invalid input device (no default output device)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-97639f0a42fa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Start Recording\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m stream = audio.open(format=FORMAT, channels=CHANNELS,\n\u001b[0m\u001b[1;32m     14\u001b[0m                     \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     frames_per_buffer=CHUNK)\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyaudio/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPyAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStream\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \"\"\"\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_streams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyaudio/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, PA_manager, rate, channels, format, input, output, input_device_index, output_device_index, frames_per_buffer, start, input_host_api_specific_stream_info, output_host_api_specific_stream_info, stream_callback)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;31m# calling pa.open returns a stream object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_latency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputLatency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno -9996] Invalid input device (no default output device)"
     ]
    }
   ],
   "source": [
    "# Cell 4: Record audio for a fixed duration\n",
    "FORMAT = pyaudio.paInt16  # Audio format (16-bit PCM)\n",
    "CHANNELS = 1              # Number of audio channels (1 for mono, 2 for stereo)\n",
    "RATE = 16000              # Sample rate (Whisper expects 16kHz)\n",
    "CHUNK = 1024              # Number of frames per buffer\n",
    "RECORD_SECONDS = 5        # Duration of recording in seconds\n",
    "WAV_FILENAME = \"recorded_audio.wav\"\n",
    "MP3_FILENAME = \"recorded_audio.mp3\"\n",
    "\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Start Recording\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                    rate=RATE, input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"Recording...\")\n",
    "frames = []\n",
    "\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "\n",
    "print(\"Finished recording.\")\n",
    "\n",
    "# Stop Recording\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n",
    "# Save the recorded data as a WAV file\n",
    "wf = wave.open(WAV_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()\n",
    "\n",
    "print(f\"Audio saved as {WAV_FILENAME}\")\n",
    "\n",
    "# Convert WAV to MP3 using pydub\n",
    "try:\n",
    "    sound = AudioSegment.from_wav(WAV_FILENAME)\n",
    "    sound.export(MP3_FILENAME, format=\"mp3\")\n",
    "    print(f\"Converted to {MP3_FILENAME}\")\n",
    "    os.remove(WAV_FILENAME) # Optional: remove the intermediate WAV file\n",
    "    # Display the audio player in the notebook\n",
    "    display(Audio(MP3_FILENAME))\n",
    "except Exception as e:\n",
    "    print(f\"Could not convert to MP3. Make sure ffmpeg is installed and in your PATH. Error: {e}\")\n",
    "    print(\"You can still use the WAV file for prediction if MP3 conversion fails, but ensure your prediction block loads the correct file type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "Vff6cvX_NJSf",
    "outputId": "0febfea5-cbbd-4ef8-df4a-374fb233fe68"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno -9996] Invalid input device (no default output device)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ac91ec607526>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m stream = audio.open(format=FORMAT, channels=CHANNELS,\n\u001b[0m\u001b[1;32m     16\u001b[0m                     \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     frames_per_buffer=CHUNK)\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyaudio/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPyAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStream\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \"\"\"\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_streams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyaudio/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, PA_manager, rate, channels, format, input, output, input_device_index, output_device_index, frames_per_buffer, start, input_host_api_specific_stream_info, output_host_api_specific_stream_info, stream_callback)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;31m# calling pa.open returns a stream object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_latency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputLatency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno -9996] Invalid input device (no default output device)"
     ]
    }
   ],
   "source": [
    "# Cell 5: Record audio (stop with Enter key in console - more for script, but can work)\n",
    "# This method will record until you press Enter in the console where Jupyter is running.\n",
    "# Note: In some environments (like Colab without specific widgets), direct key press detection is hard.\n",
    "# A simpler alternative is just a longer fixed duration.\n",
    "\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000 # Whisper expects 16kHz\n",
    "CHUNK = 1024\n",
    "WAV_FILENAME_MANUAL = \"recorded_audio_manual.wav\"\n",
    "MP3_FILENAME_MANUAL = \"recorded_audio_manual.mp3\"\n",
    "\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                    rate=RATE, input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"Recording... Press Enter in the console running Jupyter to stop.\")\n",
    "frames = []\n",
    "# This input() will block, so recording happens while it waits.\n",
    "# This isn't a perfect \"stop recording on keypress\" but a simple way to gate it.\n",
    "# For true keypress stop, you'd need a more complex GUI or threading.\n",
    "try:\n",
    "    while True: # A loop to simulate continuous recording until input\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "        # This is a crude way to check for input without blocking read too much\n",
    "        # It's not ideal. A better way is to record fixed long duration or use threads.\n",
    "        # For this example, let's make it simpler: Record for a max duration or until input.\n",
    "        # The input() below is the primary stop.\n",
    "except KeyboardInterrupt: # If you press Ctrl+C in console\n",
    "    print(\"Recording stopped by user.\")\n",
    "except Exception as e:\n",
    "    pass # Handle other potential stream errors\n",
    "\n",
    "# The following is a simpler approach for notebooks:\n",
    "# Ask user to press enter, then record for a few seconds.\n",
    "# Or, just use a fixed duration like Method 1.\n",
    "\n",
    "# Let's refine Method 2 for a notebook: record after user hits enter.\n",
    "print(\"Prepare to record. Press Enter to start recording for 10 seconds.\")\n",
    "input() # Wait for user to press Enter\n",
    "\n",
    "print(\"Recording for 10 seconds...\")\n",
    "frames = []\n",
    "for i in range(0, int(RATE / CHUNK * 10)): # Record for 10 seconds\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "print(\"Finished recording.\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n",
    "wf = wave.open(WAV_FILENAME_MANUAL, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()\n",
    "print(f\"Audio saved as {WAV_FILENAME_MANUAL}\")\n",
    "\n",
    "try:\n",
    "    sound = AudioSegment.from_wav(WAV_FILENAME_MANUAL)\n",
    "    sound.export(MP3_FILENAME_MANUAL, format=\"mp3\")\n",
    "    print(f\"Converted to {MP3_FILENAME_MANUAL}\")\n",
    "    os.remove(WAV_FILENAME_MANUAL)\n",
    "    display(Audio(MP3_FILENAME_MANUAL))\n",
    "except Exception as e:\n",
    "    print(f\"Could not convert to MP3: {e}. Using WAV.\")\n",
    "    MP3_FILENAME_MANUAL = WAV_FILENAME_MANUAL # Fallback to WAV if MP3 fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3pYHOEFNvI5"
   },
   "source": [
    "## Part 3. Geting predictions with new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diqTg76pNvQi"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Make predictions with the recorded audio\n",
    "# Ensure this uses the filename from the recording block you ran (e.g., MP3_FILENAME)\n",
    "AUDIO_TO_TRANSCRIBE = MP3_FILENAME # Or MP3_FILENAME_MANUAL if you used that block\n",
    "\n",
    "if not os.path.exists(AUDIO_TO_TRANSCRIBE):\n",
    "    print(f\"Audio file {AUDIO_TO_TRANSCRIBE} not found. Please record audio first.\")\n",
    "else:\n",
    "    print(f\"Transcribing {AUDIO_TO_TRANSCRIBE}...\")\n",
    "    # Load the audio file using librosa (handles resampling and conversion to mono float)\n",
    "    # Whisper feature_extractor expects a 1D numpy array at 16kHz.\n",
    "    speech_array, sampling_rate = librosa.load(AUDIO_TO_TRANSCRIBE, sr=16000, mono=True)\n",
    "\n",
    "    # Preprocess the audio\n",
    "    # If you loaded processor = WhisperProcessor.from_pretrained(MODEL_ID)\n",
    "    inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_features = inputs.input_features.to(DEVICE)\n",
    "    # If you loaded feature_extractor and tokenizer separately:\n",
    "    # input_features = feature_extractor(speech_array, sampling_rate=16000, return_tensors=\"pt\").input_features.to(DEVICE)\n",
    "\n",
    "\n",
    "    # Generate token IDs\n",
    "    with torch.no_grad():\n",
    "        # You might need to specify the language here if your model requires it for Guarani.\n",
    "        # Example: generated_ids = model.generate(input_features, language=\"gn\")\n",
    "        # If your model was trained with \"spanish\" as the target token:\n",
    "        # generated_ids = model.generate(input_features, language=\"spanish\")\n",
    "        # Or rely on the model's default config if set:\n",
    "        generated_ids = model.generate(input_features)\n",
    "\n",
    "    # Decode the token IDs to text\n",
    "    # If you loaded processor:\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    # If you loaded tokenizer separately:\n",
    "    # transcription = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(\"\\nTranscription:\")\n",
    "    print(transcription)\n",
    "\n",
    "    # Alternative: Using the pipeline (can be more convenient)\n",
    "    # from transformers import pipeline\n",
    "    # print(\"\\nUsing pipeline for transcription:\")\n",
    "    # # If you loaded feature_extractor and tokenizer separately, ensure processor is created for pipeline\n",
    "    # # if 'processor' not in locals():\n",
    "    # # processor = WhisperProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "    # pipe = pipeline(\"automatic-speech-recognition\",\n",
    "    #                 model=model,\n",
    "    #                 tokenizer=processor.tokenizer, # or your loaded tokenizer\n",
    "    #                 feature_extractor=processor.feature_extractor, # or your loaded feature_extractor\n",
    "    #                 device=DEVICE)\n",
    "    # result = pipe(AUDIO_TO_TRANSCRIBE, generate_kwargs={\"language\": \"gn\"}) # or \"spanish\" or remove if config is set\n",
    "    # print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RSKvi0vN3nz"
   },
   "source": [
    "## Part 4. Comparing model sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtfc-B0uN3t2"
   },
   "outputs": [],
   "source": [
    "# Cell 7: Load a specific older version of the model (your 'tiny' version)\n",
    "# Replace 'your-username/your-repo-name' with your Hugging Face model ID\n",
    "# Replace 'commit_hash_of_tiny_version' with the actual commit hash from Hugging Face\n",
    "MODEL_ID_FOR_COMPARISON = \"your-username/your-repo-name\"\n",
    "COMMIT_HASH_TINY = \"commit_hash_of_tiny_version\" # <--- PASTE THE CORRECT COMMIT HASH HERE\n",
    "\n",
    "print(f\"Loading 'tiny' version from commit: {COMMIT_HASH_TINY}\")\n",
    "\n",
    "try:\n",
    "    # Attempt to load with WhisperProcessor first\n",
    "    processor_tiny = WhisperProcessor.from_pretrained(MODEL_ID_FOR_COMPARISON, revision=COMMIT_HASH_TINY)\n",
    "    model_tiny = WhisperForConditionalGeneration.from_pretrained(MODEL_ID_FOR_COMPARISON, revision=COMMIT_HASH_TINY).to(DEVICE)\n",
    "    print(\"Successfully loaded 'tiny' model and processor using revision.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading 'tiny' processor with revision: {e}. Trying tokenizer/feature_extractor separately.\")\n",
    "    # Fallback if processor loading fails for that revision\n",
    "    tokenizer_tiny = WhisperTokenizer.from_pretrained(MODEL_ID_FOR_COMPARISON, revision=COMMIT_HASH_TINY)\n",
    "    feature_extractor_tiny = WhisperFeatureExtractor.from_pretrained(MODEL_ID_FOR_COMPARISON, revision=COMMIT_HASH_TINY)\n",
    "    model_tiny = WhisperForConditionalGeneration.from_pretrained(MODEL_ID_FOR_COMPARISON, revision=COMMIT_HASH_TINY).to(DEVICE)\n",
    "    print(\"Successfully loaded 'tiny' model, tokenizer, and feature_extractor using revision.\")\n",
    "\n",
    "\n",
    "model_tiny.eval()\n",
    "\n",
    "# Now you have 'model_tiny' (and its processor/tokenizer/feature_extractor) loaded.\n",
    "# You can compare its performance to your current 'model' (the 'small' version).\n",
    "# For example, you could re-run the prediction cell (Cell 6) but use model_tiny:\n",
    "\n",
    "# Example of using the loaded tiny model for prediction:\n",
    "# if 'speech_array' in locals(): # Check if speech_array from previous prediction exists\n",
    "#     inputs_tiny = processor_tiny(speech_array, sampling_rate=16000, return_tensors=\"pt\").input_features.to(DEVICE)\n",
    "#     # Or if using separate tokenizer/feature_extractor_tiny:\n",
    "#     # inputs_tiny = feature_extractor_tiny(speech_array, sampling_rate=16000, return_tensors=\"pt\").input_features.to(DEVICE)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         generated_ids_tiny = model_tiny.generate(inputs_tiny) # Add language if needed\n",
    "#     transcription_tiny = processor_tiny.batch_decode(generated_ids_tiny, skip_special_tokens=True)[0]\n",
    "#     # Or with separate tokenizer_tiny:\n",
    "#     # transcription_tiny = tokenizer_tiny.batch_decode(generated_ids_tiny, skip_special_tokens=True)[0]\n",
    "#     print(\"\\nTranscription from 'tiny' model:\")\n",
    "#     print(transcription_tiny)\n",
    "# else:\n",
    "#     print(\"Run audio recording and preprocessing first to compare 'tiny' model.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
